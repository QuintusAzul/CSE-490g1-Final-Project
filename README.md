# CSE-490g1-Final-Project
Team member: Tim Li, Xun Cao

![scale=0.5](fake_tweet.png)
## Abstract


## Problem statement - what are you trying to solve/do
Recently, the widespread of fake news and conspiracies have caused great damage onto the society. Fake news about the ongoing COVID-19 pandemic and anti-vaccine conspiracies has resulted in death of tens of thousands lives in US alone. Also fake news about the 2020 presidential election has resulted in the 2021 United States Capitol attack. In addition, the development in language models has enabled us to generate sentence that mimic human's writing. Thus, it became possible to generate huge amount of fake news with a press of a button.

## Related work - what papers/ideas inspired you, what datasets did you use, etc



GPT-2: <sup>[[2]](#GPT2)</sup> \
Word2Vec: <sup>[[3]](#w2v)</sup> \
dataset: <sup>[[4]](#celebrity)</sup>

sad
## Methodology - what is your approach/solution/what did you do?

To generate tweet that can fool human, we decided to build on existing state-of-the-art language models built on transformer model <sup>[[1]](#transformer)</sup> that rely on the self-attention mechanism by fine-tuning those models on a specified twitter dataset. In particular, we used two datasets: a archive of all Donald Trump tweets and Top 20 most followed users in Twitter social platform.

Our first approach is to fine-tune a GPT-2 model on the twitter datasets. We planned to combined those two dataset together and place the name of different Twitter account as seed word. Unfortunately, the result is less than ideal where due to inbalanced entries for different Twitter account where tweets from Donald Trump is significantly more than the rest.

Then, we decided to split different Twitter accounts into different fine-tuned models.

Therefore, in case of user inputs that beyond the list of Twitter accounts in our datasets. We decided to implement Word2Vec that allow us to map the customized user inputs into a suitable model. Using the existing Word2Vec models that is pretrained on Twitter corpus, we were able to use the most relevant model in our tweet generation based on user input.

## Experiments/evaluation - how are you evaluating your results
To evaluate our result, we recruited 5 volunteers to conduct a experiment. We first showed participants 50 real tweets randomly selected in our dataset as well as 50 fake tweets generated by the model while informing which one is real or not. Then, we asked the participants to classify 100 tweets into real and fake categories. In those 100 tweets, there are 50 real tweets and 50 fake tweets. However, volunteers are being told number of tweets in each category are unknown. Participants are able to search the person's identity aside from directly looking up the tweet on the internet if they are not familar with that person. Since our goal is to generate fake tweet that mimics real tweet, the accuracy of our participants is inversely correlated with the efficacy of our models.

## Results - How well did you do

![scale=0.5](experiment%20result.png)

In our statistical analysis, the null hypothesis is people can identify real and fake tweets with great accuracy. And the alternative hypothesis is that people cannot achieve that accuracy.\
<img src="https://latex.codecogs.com/png.latex?%5Cdpi%7B200%7D%20%5Cbg_white%20%5Csmall%20H_0%3A%20p%3E0.75" width="150"> \
<img src="https://latex.codecogs.com/png.latex?%5Cdpi%7B200%7D%20%5Cbg_white%20%5Csmall%20H_1%3A%20p%3C%3D0.75" width="150">

Thus, we performed a Welch Two Sample t-test on the given statistics. The results are as follows: <img src="test.png" alt="test" width="450"/>

Thus, at the significant level of 0.05, there are statistically signficant evidence that support the alternative hypothesis that people cannot identify real and fake tweets with great accuracy.


## Examples - images/text/live demo, anything to show off your work

## Video - a 2-3 minute long video where you explain your project and the above information

## References:

<a name="transformer">[1]</a>: [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\
<a name="GPT2">[2]</a>: [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\
<a name="w2v">[3]</a>: [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781)\
<a name="celebrity">[4]</a>: [Bin Tareaf, Raad, 2017, "Tweets Dataset - Top 20 most followed users in Twitter social platform", Harvard Dataverse, V2](https://doi.org/10.7910/DVN/JBXKFD)\
<a name="report">[5]</a>: [Disinformation, ‘Fake News’ and Influence Campaigns on Twitter](https://s3.amazonaws.com/kf-site-legacy-media/feature_assets/www/misinfo/kf-disinformation-report.0cdbb232.pdf)
